말씀하신 대로 **"화자의 얼굴 표정(Speaker Visual)"** 입력까지 포함하여, **[Listening 모드 집중 & 단계별 검증]**을 위한 최종 수정된 마스터 플랜을 전달해 드립니다.

Brain(두뇌)이 오디오뿐만 아니라 화자의 표정까지 읽어내어 **"침묵 속의 미소"**나 **"분위기 파악"**까지 가능한 고지능 리스너를 만들기 위한 설계도입니다.

---

# **🚀 Diff-Listener 개발 마스터 플랜 (2주 완성)**

*(Updated: Speaker Visual Input 추가됨)*

## **Phase 0. 데이터 준비 및 전처리 (Day 1~2)**

**목표:** Body(Macron)와 Brain(ViCo) 학습을 위한 데이터를 **"기계가 먹을 수 있는 숫자"**로 변환하기.

### **1. Body 학습용 데이터 (Macron/May)**

* **Golden Clip 선정:** 마이크가 턱을 가리지 않고, 정면 위주지만 고개를 살짝 돌리는 3~5분 클립.
* **Listener Visual (GT) 추출:**
* **도구:** EMOCA (추천) 또는 MediaPipe.
* **출력:** `.npy` 파일 (Shape: `[Total_Frames, 56]`).
* **구성:** `Expression (50)` + `Pose (6: Jaw + Neck + Head)`.



### **2. Brain 학습용 데이터 (ViCo 데이터셋)**

* **Listener Visual (Target) 추출:** 위와 동일 (Brain이 맞춰야 할 정답).
* **Speaker Audio (Input 1) 추출:**
* **도구:** Wav2Vec 2.0.
* **출력:** `.npy` 파일 (Shape: `[Total_Frames, 768]`).
* **⭐ Expert Tip 1 (Audio Resampling):** Wav2Vec은 보통 16kHz 오디오를 입력받습니다. 오디오 파일의 Sample Rate를 꼭 맞추고, 비디오 프레임 수와 1:1 매칭되는지(Interpolation 필요 여부) 확인하세요.


* **✨ Speaker Visual (Input 2) 추출 [추가됨]:**
* **대상:** 화자의 얼굴.
* **도구:** EMOCA.
* **출력:** `.npy` 파일 (Shape: `[Total_Frames, 50]`).
* **구성:** 오직 **`Expression (50)`**만 추출. (화자의 고개 각도나 얼굴형은 감정 읽기에 불필요하므로 데이터 절약).
* **⭐ Expert Tip 2 (Sync Check):** `Audio(768)` + `Speaker Visual(50)` + `Listener GT(56)` 이 세 가지 데이터의 프레임 수가 완벽하게 일치해야 합니다. 하나라도 밀리면 학습이 망가집니다.



---

## **Phase 1. Body 수술 및 자가복제 검증 (Day 3~5)**

**목표:** TalkingGaussian을 **"파라미터 입력형 렌더러"**로 개조하고, Macron 얼굴을 학습시킴. (입력 조건이 늘어나도 Body는 수정할 필요 없음)

1. **코드 개조 (`deformation_network.py`):**
* `AudioEncoder` 모듈 삭제.
* `forward()` 함수의 입력 인자를 `audio`에서 `flame_params`로 변경.
* 첫 번째 Layer 수정: `nn.Linear(audio_dim, hidden)`  `nn.Linear(56, hidden)`.


2. **오버피팅 학습 (Training):**
* **입력:** Macron GT FLAME Params (56-dim).
* **정답:** Macron 원본 이미지.
* **⭐ Expert Tip 3 (Normalization):** FLAME 파라미터 값들을 `Mean/Std` 정규화하여 0~1 사이 분포로 만들어주세요. 학습 속도가 빨라집니다.


3. **검증 (Reconstruction Test):**
* 학습된 모델에 GT Params를 다시 넣어서 비디오 생성.
* **Pass 기준:** 원본과 나란히 놨을 때, 입 모양과 눈 깜빡임이 **95% 이상 일치**해야 함.



---

## **Phase 2. 스트레스 테스트 (The "Macron Risk" Check) (Day 6)**

**목표:** 점잖은 Macron 모델이 **"격한 감정"**을 소화할 수 있는지 확인.

1. **외부 데이터 준비:** 유튜브 등에서 "입을 크게 벌리고 웃는 배우" 영상에서 파라미터 추출.
2. **주입 및 관찰:** 격한 파라미터를 Macron Body에 주입.
3. **결과 판독:**
* **Case A (성공):** 입 모양이 안 깨짐  **Go**.
* **Case B (실패):** 입술 찢어짐/구멍 뚫림  **Stop**. (Macron 데이터 버리고 표정 풍부한 데이터로 Phase 1 재수행).



---

## **Phase 3. Brain 이식 및 오버피팅 (Day 7~10)**

**목표:** 오디오와 **[화자 표정]**을 보면 리액션을 생성하는 지능(Brain) 만들기.

1. **모델 구축 (Simple MLP Diffusion):**
* 초기엔 5-Layer MLP + Residual Connection 추천.
* **입력 차원:** `Noisy Params(56)` + `Audio(768)` + **`Speaker Visual(50)`** + `Time(Embed)`.
* **총 컨디션 차원:** 약 818차원 (768+50).


2. **1차 시도: Regression (Prior Check):**
* **⭐ Expert Tip 4 (디버깅):** Diffusion 전에 단순 회귀(`Inputs -> Listener Params`)로 먼저 학습해 봅니다. 오디오와 화자 표정을 넣었을 때 리스너가 반응하는 척이라도 하면 성공입니다.


3. **2차 시도: Diffusion Overfitting:**
* 모델을 Diffusion 구조(Noise 예측)로 변경.
* ViCo 데이터 중 일부 클립으로 오버피팅 수행.
* **검증:** 학습 데이터의 오디오+화자표정을 넣었을 때, GT 리스너 파라미터와 유사한 그래프가 나오는지 확인.



---

## **Phase 4. 확장 및 최종 튜닝 (Day 11~14)**

**목표:** 다양한 사람들의 리액션을 배워서 Macron에게 이식하고, Listening 모드 완성.

1. **Brain 업그레이드 (Generalization):**
* ViCo 데이터셋 전체(다양한 화자-청자 쌍)로 Brain 학습.
* 화자가 웃을 때(Visual Input) 청자가 같이 웃는지(Output) 확인 (**Social Mimicry 검증**).
* **⭐ Expert Tip 5 (Domain Adaptation):** Brain이 생성한 파라미터 스케일이 Macron Body의 허용 범위를 넘지 않도록, Inference 단계에서 **Rescaling(Min-Max 매핑)** 후처리가 필수적입니다.


2. **스무딩 (Post-processing):**
* `Gaussian Smoothing` 등을 적용해 떨림 제거.


3. **최종 데모 생성:**
* `New Speaker Audio/Visual`  **Brain**  `Params`  **Body**  Video.



---

## **[부록] Speaking 모드 (립싱크) 보존 전략**

Listening 검증이 끝난 후, 여유가 되면 적용할 하이브리드 전략입니다.

* **배경:** 3DGS(Body) 내부에 Audio Encoder를 살려두면 구조가 복잡해집니다.
* **해결책 (Adapter 패턴):**
* Body는 무조건 **Feature Vector(예: 64-dim)**만 받도록 고정합니다.
* **Speaking 시:** `Wav2Lip` 스타일의 오디오 인코더가 `Audio -> Feature` 변환.
* **Listening 시:** `Diffusion Brain`이 `Params` 생성  `MLP`가 `Params -> Feature` 변환.
* 이렇게 하면 Body 재학습 없이 입구(Adapter)만 갈아끼워 모드 전환이 가능합니다.



**이제 설계도는 완벽합니다. Phase 0의 데이터셋 전처리(화자 표정 포함)부터 바로 시작하시면 됩니다!**
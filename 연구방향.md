지금까지 논의한 내용을 바탕으로, **ICIP 투고를 위한 "Diff-Listener" 연구 방향**을 완벽하게 정리해 드립니다. 이 내용을 연구 노트나 논문 초안의 뼈대로 삼으시면 됩니다.

---

### **📌 연구 제목 (가제)**

**"Diff-Listener: Probabilistic 3D Listening Head Generation via Diffusion-based Gaussian Splatting"**
*(Diff-Listener: 확산 기반 가우시안 스플래팅을 이용한 확률적 3D 청자 생성)*

---

### **1. 문제 인식 (Motivation)**

기존 연구들의 한계를 지적하며 **"왜 이 연구가 필요한가?"**를 설득합니다.

1. **2D 모델의 한계 (vs LivePortrait):** 2D 워핑 방식은 고개를 크게 돌리거나 격하게 반응할 때(Large Pose) 얼굴 외곽이 무너지거나 아티팩트가 생긴다.  **3DGS로 해결.**
2. **결정론적 모델의 한계 (vs Regression):** 기존 모델(L2L 등)은 같은 말을 들으면 항상 똑같은 표정만 짓는다. 기계적이고 지루하다.  **Diffusion으로 다양성(Diversity) 확보.** 


3. **오디오 의존성의 한계:** 오디오만으로는 '비꼬는 표정'이나 '침묵 속의 미소'를 파악하기 어렵다.  **Speaker Visual 정보 추가.**

---

### **2. 제안 방법론 (Proposed Method)**

시스템은 크게 **'두뇌(Brain)'**와 **'몸체(Body)'**로 나뉩니다.

#### **A. 두뇌: Probabilistic Motion Generator (Diffusion)**

* 
**역할:** 화자의 정보를 듣고/보고, **"어떤 표정()과 자세()를 취할지"** 결정하여 **FLAME 파라미터(숫자 벡터)**를 생성함. 


* **모델 구조:** **1D Diffusion Transformer** (INFP Stage 2 또는 ARIG 구조 변형).
* 이미지 생성이 아닌, **시계열 벡터(Time-series Vector)** 생성 모델. 




* **입력 (Multi-modal inputs):**
1. 
**Speaker Audio:** Wav2Vec 2.0 피처 (768차원). 


2. **Speaker Visual:** 화자의 표정(Expression Code, 50차원) from EMOCA. (오디오의 빈틈 보완). 


3. **Listener History:** 이전 프레임의 내 움직임 (연속성).


* 
**출력:** Listener FLAME Parameters (Expression 50 + Pose 6 = 약 56차원). 



#### **B. 몸체: High-Fidelity 3D Renderer (3DGS)**

* **역할:** 두뇌가 준 파라미터를 받아 **"실제 고화질 얼굴"**을 그리기.
* 
**베이스 모델:** **TalkingGaussian** (GaussianTalker 아님). 


* *선택 이유:* 리스너에게 필수적인 **미세 표정(Fine-grained Expression)** 묘사에 더 유리하며 구조가 모듈화되어 있음.


* 
**수정 사항:** 기존 오디오 인코더를 제거하고, **FLAME 파라미터**를 변형장(Deformation Field)의 입력으로 받도록 개조. 



---

### **3. 학습 전략 (Two-Stage Training)**

전체를 한 번에 학습하지 않고 단계별로 진행하여 안정성을 높입니다. 

* 
**준비 (Preprocessing):** **ViCo 데이터셋**에서 EMOCA를 이용해 **Ground Truth FLAME 파라미터** 추출. 


* **Stage 1 (Body):**
* GT 파라미터를 넣으면 원본 얼굴이 나오도록 **TalkingGaussian 렌더러**만 학습.
* 학습 후 Weight 고정(Freeze).


* **Stage 2 (Brain):**
* 오디오+비주얼 입력을 주면 GT 파라미터를 생성하도록 **Diffusion 모델**만 학습. 


* Loss: MSE (Parameter Space). 





---

### **4. 핵심 구현 포인트 (Implementation details)**

1. **Identity:** 3DGS 특성상 Identity는 학습된 체크포인트 파일에 저장됨. (Subject-specific). Diffusion은 "움직임"만 배움. 


2. **Latency 방어:** **Text(LLM/BERT) 사용 안 함.**
* *논리:* ASR 지연 시간 때문에 실시간성이 깨짐. 대신 Wav2Vec 2.0이 언어적 특징을 암시적으로 알고 있다고 주장. 




3. 
**데이터:** **ViCo Dataset** 사용 (리스너 연구 표준). 


4. 
**Diffusion:** 추론 시 **15 Step** 정도의 DDIM 사용 (속도 확보). 


5. 
**입력 처리:** 화자의 얼굴 전체 이미지가 아니라 **파라미터(Code)**만 뽑아서 입력으로 씀 (가볍게 유지). 



---

### **5. 예상되는 논문 기여점 (Contribution)**

1. 
**High-Fidelity:** 3DGS를 리스너 생성에 도입하여 2D 모델의 왜곡 문제를 해결하고 512x512 이상의 고화질 달성. 


2. 
**Diverse & Natural:** Diffusion을 통해 회귀 모델의 고질병인 '평균적이고 밋밋한 반응'을 극복하고 다양한 리액션 생성. 


3. 
**Multi-modal Awareness:** 오디오뿐만 아니라 화자의 표정까지 고려하여, 침묵 상황에서도 적절한 반응(Social Mimicry) 생성. 



---

### **🏃‍♂️ 당장 시작해야 할 Action Plan**

1. 
**데이터 파이프라인:** ViCo 데이터셋 다운로드  **EMOCA**로 전체 프레임 FLAME 파라미터 추출 (`.npy` 저장). 


2. 
**몸체 개조:** TalkingGaussian 코드에서 `Audio Encoder` 떼어내고 `Parameter Input`으로 변경 후 렌더링 테스트. 


3. 
**두뇌 구현:** `diffusion_policy` 또는 Transformer 기반 1D Diffusion 코드 작성  Stage 1 데이터로 학습. 



이 방향이면 논리적 구멍 없이 ICIP 4페이지를 아주 알차게 채우실 수 있습니다. 지금 바로 데이터셋 다운로드부터 시작하십시오!
오버피팅(Overfitting)을 통해 **"이 구조가 진짜 돌아가긴 하는지"** 빠르게 검증하겠다는 전략은 **연구 초기 단계에서 가장 효율적인 접근법**입니다. 데이터 탓인지 모델 탓인지 헷갈릴 일을 원천 차단할 수 있기 때문입니다.

가장 빠르고 확실하게 **"Diff-Listener MVP (Minimum Viable Product)"**를 띄울 수 있는 **초단기(2주) 액션 플랜**을 짜드립니다.

---

### **🎯 목표: "단 한 명의 리스너, 단 1분의 비디오라도 완벽하게 동작시키기"**

* **전략:** 일반화(Generalization) 포기  **오버피팅(Overfitting) 올인**.
* **데이터:** ViCo 데이터셋 중 **가장 화질 좋고 정면을 보는 1개 클립(약 1~3분)**만 사용.

---

### **[Phase 1] 데이터 준비: "딱 1개만 팬다" (Day 1~2)**

복잡한 데이터 로더 짤 필요 없이, 딱 하나의 비디오 폴더만 처리합니다.

1. **Golden Clip 선정:**
* ViCo 데이터셋에서 **화자가 말하고 있고 청자가 잘 듣고 있는** 클립 하나를 고르십시오. (청자 얼굴이 잘 보여야 함).


2. **GT 추출 (EMOCA):**
* 선정한 비디오(Listener Video)를 프레임별로 잘라서 이미지 저장.
* 
`EMOCA`  (또는 MediaPipe)를 돌려서 각 프레임의 **FLAME 파라미터(Expression 50 + Pose 6)**를 추출하여 `.npy`로 저장합니다.


* 이 `.npy` 파일이 이제 **절대적인 정답지**입니다.


3. **오디오 추출:**
* 해당 클립의 오디오(`.wav`)를 `Wav2Vec 2.0` 에 통과시켜 프레임별 피처(`.npy`)를 뽑아둡니다.





> **✅ Checkpoint:** `Audio Feature(T, 768)`와 `FLAME Params(T, 56)`의 **길이(T)**가 딱 맞는지 확인하세요.

---

### **[Phase 2] 몸체 수술: TalkingGaussian 개조 (Day 3~6)**

TalkingGaussian이 오디오 대신 **파라미터**를 받아먹도록 입만 바꿉니다.

1. **입력부 수정 (`deformation_network.py`):**
* 기존: `input_dim = audio_dim (e.g., 64)`
* 변경: `input_dim = flame_param_dim (e.g., 56)`
* 오디오 인코더(`AudioEncoder`) 모듈은 주석 처리해서 꺼버리세요.


2. **오버피팅 학습 (`train.py`):**
* 데이터 로더에서 오디오 대신 위에서 뽑은 **GT FLAME Params**를 넣어줍니다.
* **학습:** 준비한 **단 1개의 비디오**로 렌더러를 학습시킵니다.
* **기대 효과:** 모델이 그 사람의 얼굴, 점, 주름까지 완벽하게 외워버립니다.


3. **검증 (Reconstruction Test):**
* 학습된 렌더러에 **GT FLAME Params**를 다시 넣어봅니다.
* 원본 비디오와 똑같은 영상이 나오면 **Body 완성**입니다.



---

### **[Phase 3] 두뇌 이식: 초간단 Diffusion 구현 (Day 7~10)**

복잡한 Transformer 말고, 가장 단순한 **MLP Diffusion**으로 시작합니다.

1. **모델 구현 (`brain.py`):**
* `diffusers` 라이브러리가 있다면 `DDPMScheduler`만 가져오고, 모델은 직접 짭니다.
* **구조:** `Linear(Input) -> ReLU -> Linear ... -> Linear(Output)` 형태의 5~6층짜리 MLP. (Residual Connection 추가하면 좋음) .


* **입력:** `Noisy Params(56)` + `Audio Condition(768)` + `Timestep Embedding`.
* **출력:** `Predicted Noise(56)`.


2. **오버피팅 학습:**
* Phase 1의 오디오/파라미터 데이터를 그대로 씁니다.
* **Loss:** MSE Loss.
* **Epoch:** Loss가 0에 수렴할 때까지 무식하게 돌립니다 (Overfitting).


3. **검증 (Generation Test):**
* 학습 때 썼던 오디오를 다시 입력으로 줍니다.
* Diffusion이 생성한 파라미터가 **GT 파라미터와 그래프 모양이 거의 겹치는지** 확인합니다. (완벽히 겹치면 Overfitting 성공).



---

### **[Phase 4] 합체 및 시연 (Day 11~14)**

두 모델을 파이프라인으로 연결합니다.

1. **Inference Script (`run_demo.py`):**
* `Audio` 로드  **Brain (Diffusion)**  `Pred Params` 생성 (15 step DDIM).
* `Pred Params`  **Body (TalkingGaussian)**  `Final Video` 렌더링.


2. **시각화:**
* 원본 리스너 비디오(GT)와 생성된 비디오(Generated)를 나란히(Side-by-side) 붙여서 비교합니다.
* 오버피팅이 잘 됐다면 육안으로 구분이 어려울 정도로 똑같이 따라 해야 합니다.



---

### **💡 팁: 실패 확률 줄이기**

* **Brain 학습 시:** 처음엔 **Diffusion도 쓰지 말고 그냥 MLP (Regression)**로 먼저 해보세요. 오디오 넣어서 파라미터 값이 얼추 맞는지 확인한 후 Diffusion으로 바꾸면 디버깅이 훨씬 쉽습니다.
* **Body 학습 시:** TalkingGaussian은 학습이 꽤 빠릅니다. 1개 비디오라면 몇 시간 안에 결과가 나옵니다.
* **검증 우선:** "새로운 오디오(Test set)"에 잘 작동하는지는 나중 문제입니다. 우선 **"봤던 거라도 잘 따라 하는지(Train set Performance)"** 확인하는 게 이 단계의 핵심입니다.

이 플랜대로 2주만 집중하시면, **"Diffusion으로 제어되는 3D 리스너"**의 작동 가능한 프로토타입을 손에 쥐실 수 있습니다.